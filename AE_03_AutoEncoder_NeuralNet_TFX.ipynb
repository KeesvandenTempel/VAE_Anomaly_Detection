{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "TFX version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"NNJZ\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvdte\\My Drive (kvdtempel@gmail.com)\\AILabs\\JUP_AutoEncoder_JZ\\data/\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(base_dir, \"data/\")\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvdte\\AppData\\Local\\Temp\\tfx-dataa2fkxv8x\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Maak eerst een tijdelijke directory, verkrijg de dataset m.b.v. een Uri, en maakeen kopie naar de tijdelijke dir\n",
    "# urllib.request.urlretrieve(url, filename=None, reporthook=None, data=None)\n",
    "# Copy a network object denoted by a URL to a local file. If the URL points to a local file, the object \n",
    "# will not be copied unless filename is supplied. Return a tuple (filename, headers) where filename is the local file\n",
    "\n",
    "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
    "_data_url = 'file:///Users/kvdte/My Drive (kvdtempel@gmail.com)/AILabs/JUP_AutoEncoder_JZ/data/NN_Declaraties_autoencoder_train.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
    "urllib.request.urlretrieve(_data_url, _data_filepath)\n",
    "print(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:InteractiveContext pipeline_root argument not provided: using temporary directory C:\\Users\\kvdte\\AppData\\Local\\Temp\\tfx-interactive-2023-02-13T22_19_37.568094-dz8d8vhu as root for pipeline outputs.\n",
      "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at C:\\Users\\kvdte\\AppData\\Local\\Temp\\tfx-interactive-2023-02-13T22_19_37.568094-dz8d8vhu\\metadata.sqlite.\n",
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:Running executor for CsvExampleGen\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data tfxdata/* to TFExample.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Running publisher for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Running driver for StatisticsGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Running executor for StatisticsGen\n",
      "INFO:absl:Generating statistics for split train.\n",
      "INFO:absl:Statistics for split train written to C:\\Users\\kvdte\\AppData\\Local\\Temp\\tfx-interactive-2023-02-13T22_19_37.568094-dz8d8vhu\\StatisticsGen\\statistics\\2\\Split-train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to C:\\Users\\kvdte\\AppData\\Local\\Temp\\tfx-interactive-2023-02-13T22_19_37.568094-dz8d8vhu\\StatisticsGen\\statistics\\2\\Split-eval.\n",
      "INFO:absl:Running publisher for StatisticsGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'InteractiveContext' object has no attribute 'get_component'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13196\\472088158.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Get the statistics from the InteractiveContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mstatistics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_component\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatistics_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'statistics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Get the statistics from the StatisticsGen component\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InteractiveContext' object has no attribute 'get_component'"
     ]
    }
   ],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import pandas as pd\n",
    "\n",
    "# Create an instance of the InteractiveContext\n",
    "context = InteractiveContext()\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"data/NN_Declaraties_autoencoder_train.csv\", decimal=\",\", sep=\";\")\n",
    "data.to_csv('tfxdata/data.csv')\n",
    "\n",
    "# Create an instance of the CsvExampleGen component\n",
    "csv_example_gen = tfx.components.CsvExampleGen(input_base='tfxdata/')\n",
    "\n",
    "# Get the examples from the CsvExampleGen component\n",
    "# example = csv_example_gen.outputs['examples'].get()\n",
    "context.run(csv_example_gen)\n",
    "\n",
    "# Create an instance of the StatisticsGen component\n",
    "statistics_gen = tfx.components.StatisticsGen(examples=csv_example_gen.outputs['examples'])\n",
    "\n",
    "# Execute the components using the InteractiveContext\n",
    "context.run(statistics_gen)\n",
    "\n",
    "# Get the statistics from the InteractiveContext\n",
    "statistics = statistics_gen.outputs['statistics'].get()\n",
    "\n",
    "# Display the generated statistics\n",
    "print(statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Artifact(artifact: id: 2\n",
      "type_id: 16\n",
      "uri: \"C:\\\\Users\\\\kvdte\\\\AppData\\\\Local\\\\Temp\\\\tfx-interactive-2023-02-13T22_19_37.568094-dz8d8vhu\\\\StatisticsGen\\\\statistics\\\\2\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"statistics:2023-02-13T22:22:43.671429\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"producer_component\"\n",
      "  value {\n",
      "    string_value: \"StatisticsGen\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"statistics:2023-02-13T22:22:43.671429\"\n",
      ", artifact_type: id: 16\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "statistics = statistics_gen.outputs['statistics'].get()\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Jupyter notebook Python code voor Autoencoder vraagstuk\n",
    "# Auteur Kees van den Tempel, AI-labs BV, Tricht\n",
    "# Versie 1.0\n",
    "# 25-10-2022\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn\n",
    "import datetime\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "####*IMPORANT*: Have to do this line *before* importing tensorflow\n",
    "os.environ['PYTHONHASHSEED']=str(2)\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "#%matplotlib inline\n",
    "\n",
    "# lees de bestanden met afbeeldingen met digits in\n",
    "x_train = pd.read_csv(\"data/NN_Declaraties_autoencoder_train.csv\", decimal=\",\", sep=\";\")\n",
    "x_test = pd.read_csv(\"data/NN_Declaraties_autoencoder_test.csv\", decimal=\",\", sep=\";\")\n",
    "\n",
    "print(\"De x_train dataset bevat \" + str(x_train.shape) + \" records\")\n",
    "print(\"De x_train dataset bevat \" + str(x_train.isnull().values.sum()) + \" Null-waarden\\n\")\n",
    "print(\"De x_test dataset bevat \" + str(x_test.shape) + \" records\")\n",
    "print(\"De x_test dataset bevat \" + str(x_test.isnull().values.sum()) + \" Null-waarden\\n\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variabelen het verbeteren van de dataset\n",
    "\n",
    "# variabelen voor het optimaliseren van het algoritme\n",
    "HyperparameterTuning = False\n",
    "# Boolean om te bepalen of kFold wordt gebruikt\n",
    "UseKFold = False\n",
    "# Aantal kFold iteraties\n",
    "kKfold = 6\n",
    "DevSet = 0\n",
    "\n",
    "# Tensorboards uitproberen\n",
    "# Dropout = 0.1\n",
    "# Bottleneck = helft van de input layer units of percentage (zeg 80%) van oorspronkelijke units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute onderstaande statement in de anaconda terminal\n",
    "# python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
    "# Gaf foutmelding dat er bepaalde dlls niet gevonden konden worden\n",
    "# Computer vaak herstarten na elke installatie\n",
    "# Niet vindbare dll's in de map c:\\windows\\system32 kopieren gaf de oplossing\n",
    "\n",
    "# Python version:  3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
    "# Tensorflow version:  2.10.0\n",
    "# Keras version:  2.10.0\n",
    "# Cudnn version:  64_8\n",
    "# Cuda version:  64_112\n",
    "# Eager mode:  True\n",
    "# GPU is available\n",
    "# Num GPUs: 1\n",
    "# 1 Physical GPUs, 1 Logical GPUs\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.platform.build_info as build\n",
    "from tensorflow import keras\n",
    "\n",
    "version = tf.__version__\n",
    "executing_eagerly = tf.executing_eagerly()\n",
    "available = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"Tensorflow version: \", version)\n",
    "print(\"Keras version: \", keras.__version__)\n",
    "print(\"Cudnn version: \", build.build_info['cudnn_version'])\n",
    "print(\"Cuda version: \", build.build_info['cuda_version'])\n",
    "print(\"Eager mode: \", executing_eagerly)\n",
    "print(\"GPU is\", \"available\" if available else \"NOT AVAILABLE\")\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(2)\n",
    "   tf.random.set_seed(2)\n",
    "   np.random.seed(2)\n",
    "   rd.seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteVars(df):\n",
    "    df.drop(['Unnamed: 0', 'DeclaratieId'], axis=1, inplace=True)\n",
    "\n",
    "    volgnummers = [var for var in df.columns if '__' in var]\n",
    "    print(volgnummers)\n",
    "    df.drop(volgnummers, axis=1, inplace=True)\n",
    "\n",
    "    #gemeenten = [var for var in df.columns if 'Gemeente' in var]\n",
    "    #print(gemeenten)\n",
    "    #df.drop(gemeenten, axis=1, inplace=True)\n",
    "\n",
    "    #client = [var for var in df.columns if 'Client' in var]\n",
    "    #print(client)\n",
    "    #df.drop(client, axis=1, inplace=True)\n",
    "\n",
    "    contract = [var for var in df.columns if 'ContractId' in var]\n",
    "    print(contract)\n",
    "    df.drop(contract, axis=1, inplace=True)\n",
    "\n",
    "DID_train = x_train['DeclaratieId']\n",
    "DID_test = x_test['DeclaratieId']\n",
    "DeleteVars(x_train)\n",
    "DeleteVars(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splits de digits-dataset in een training- en een testset voor het machine learning model\n",
    "# Eerst moet er een training en test set gemaakt worden om daarna allerlei bewerkingen en filters toe te passen\n",
    "# om zo data leakage te voorkomen\n",
    "   \n",
    "print(str(type(x_train)) + ': x_train Shape:', x_train.shape)\n",
    "print(str(type(x_test)) + ': x_test Shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schakel over van panda.dataframes naar numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "print (\"X-train (de kenmerken)\" + str(x_train.shape))\n",
    "print (\"X-test (de test kenmerken)\" + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and dev-sets\n",
    "if DevSet > 0:\n",
    "    x_train, x_dev = train_test_split(x_train, test_size = DevSet, random_state = 42)\n",
    "    print('Dimensie van de trainingset x_train:' + str (x_train.shape))\n",
    "    print('Dimensie van de testset x_test:' + str(x_test.shape))\n",
    "    print('Dimensie van de dev-set x_dev:' + str(x_dev.shape) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# DIT ZIJN METRICS VOOR CLASSIFICATION PROBLEMS\n",
    "# https://medium.com/analytics-vidhya/evaluation-metrics-for-classification-problems-with-implementation-in-python-a20193b4f2c3\n",
    "# https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html\n",
    "# https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide\n",
    "# https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "\n",
    "def DisplayMetrics(rf, y_test, y_rfpred, makeplots, titel):\n",
    "    print(\"\\n\" + titel)\n",
    "    print(\"Accuracy (goed-voorspeld/totaal) (niet voor unbalanced dataset): \" + str(accuracy_score(y_test, y_rfpred).round(3)))\n",
    "    print(f\"Precision Score is (TP/TP+FP): {precision_score(y_test, y_rfpred).round(3)}\")\n",
    "    print(f\"Recall Score is (TP/TP+FN): {recall_score(y_test, y_rfpred).round(3)}\")\n",
    "    print(f\"F1 Score is (2*P*R/P+R): {f1_score(y_test, y_rfpred).round(3)}\")    \n",
    "    print(\"Random Forests roc-auc (bin class): {}\".format(roc_auc_score(y_test, y_rfpred).round(3)))\n",
    "\n",
    "    if makeplots == True:\n",
    "        matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "        \n",
    "        # confusion_matrix function a matrix containing the summary of predictions\n",
    "        cm = confusion_matrix(y_test, y_rfpred, labels=rf.classes_)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "\n",
    "        # Plotting the ROC\n",
    "        fpr, tpr, threshold = roc_curve(y_test, y_rfpred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "        \n",
    "    # FOR DEEP LEARNING CLASSIFICATION PROBLEMS\n",
    "    # print(\"\\nCLASSIFICATION METRICS\")\n",
    "    # print(keras.losses.categorical_crossentropy(y_test, y_pred, from_logits=False, label_smoothing=0))\n",
    "    # print(\"\\nCATEGORICAL ACCURACY\")\n",
    "    # print(\"Keras Accuracy: \" + str(keras.metrics.categorical_accuracy(y_test, y_pred)))\n",
    "    # print(keras.metrics.top_k_categorical_accuracy(y_test, y_pred, k=5))\n",
    "    # x_pred_max = np.argmax(x_pred, axis=1)\n",
    "    # x_test_max = np.argmax(x_test, axis=1)\n",
    "    # score = accuracy_score(x_test_max, x_pred_max)\n",
    "    # print (\"\\nAccuracy argmax: \" + str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NEURAAL NETWERK\n",
    "# Train dezelfde gegevens met een eenvoudig neuraal netwerk\n",
    "# Dit is een multiclassification probleem. De softmax functie  levert de kans op (in vector vorm) dat \n",
    "# de test-waardes op een hot-encoded (werkelijke) vector lijken. Dit wordt gemeten\n",
    "# met cross-entropy: https://www.youtube.com/watch?v=tRsSi_sqXjI\n",
    "#\n",
    "# LEES DIT : https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks\n",
    "# EN DIT: https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "# https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086\n",
    "# https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/\n",
    "# \n",
    "# --- LeNet implementatie ---\n",
    "# Convolution #1. Input = 32x32x1. Output = 28x28x6 conv2d\n",
    "# SubSampling #1. Input = 28x28x6. Output = 14x14x6. SubSampling is simply Average Pooling so we use avg_pool\n",
    "# Convolution #2. Input = 14x14x6. Output = 10x10x16 conv2d\n",
    "# SubSampling #2. Input = 10x10x16. Output = 5x5x16 avg_pool\n",
    "# Fully Connected #1. Input = 5x5x16. Output = 120\n",
    "# Fully Connected #2. Input = 120. Output = 84\n",
    "# Output 10\n",
    "\n",
    "# https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "# https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, BatchNormalization, Conv2D, Input, MaxPool2D,Flatten, Permute, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras import initializers, activations\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import CosineSimilarity\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "def SelectOptimizer(i, lr):\n",
    "    if i==1:\n",
    "        tmp = Adam(learning_rate=lr, epsilon=1e-07)\n",
    "    elif i==2:\n",
    "        tmp = Nadam(learning_rate=lr, epsilon=1e-07)\n",
    "    elif i==3:\n",
    "        tmp = SGD(learning_rate=lr, momentum=0.8, nesterov=False)\n",
    "    elif i==4:\n",
    "        tmp = SGD(learning_rate=lr, momentum=0.8, nesterov=True)\n",
    "    elif i==5:\n",
    "        tmp = RMSprop(learning_rate=lr, momentum=0.8, epsilon=1e-07)\n",
    "    return tmp\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis = None))\n",
    "\n",
    "\n",
    "def NeuralNetworkModel(activation, Param1, Param2, Param3, Param4, Param5, reg, DropoutVal, lr, OptIndex):\n",
    "    # Selecteer een optimizer\n",
    "    opt = SelectOptimizer(OptIndex, lr)\n",
    "    # tf.random.set_seed(1234)\n",
    "    \n",
    "    # Script gaf een foutmelding: onderstaande om foutmelding te verhelpen\n",
    "    # https://stackoverflow.com/questions/73978774/how-to-get-rid-of-the-userwarning-the-initializer-glorotuniform-is-unseeded-m\n",
    "    #initializer = initializers.RandomNormal(mean=0.0, stddev=0.5)\n",
    "    #initializer = initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "    #initializer = initializers.Zeros()\n",
    "    #initializer = initializers.Ones()\n",
    "    #initializer = initializers.GlorotNormal()\n",
    "    initializer = initializers.GlorotUniform(seed=0)    #Xavier = default\n",
    "    #initializer = initializers.Identity()\n",
    "    \n",
    "    kregularizer=regularizers.L1(reg)\n",
    "    #kregularizer=l1_l2(l1=0.00001, l2=0.0001),\n",
    "    bregularizer=l2(1e-4),\n",
    "    aregularizer=l2(1e-5)\n",
    "    #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    # INPUT LAYER 1: default Param1 = 82\n",
    "    model = tf.keras.models.Sequential()    \n",
    "    #model.add(tf.keras.Input(shape=(x_train.shape[1],)))\n",
    "    \n",
    "    # HIDDEN LAYER 1: default Param1 = 60\n",
    "    if Param1>0:\n",
    "        if reg>0:\n",
    "             model.add(tf.keras.layers.Dense(units=Param1, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "        else:\n",
    "             model.add(tf.keras.layers.Dense(units=Param1, kernel_initializer=initializer))\n",
    "        model.add(Activation(activation))\n",
    "        if DropoutVal>0:\n",
    "            model.add(Dropout(DropoutVal))\n",
    "    \n",
    "    # HIDDEN LAYER 2: default Param2 = 42\n",
    "    if reg>0:\n",
    "         model.add(tf.keras.layers.Dense(units=Param2, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "    else:\n",
    "         model.add(tf.keras.layers.Dense(units=Param2, kernel_initializer=initializer))\n",
    "    model.add(Activation(activation))\n",
    "    if DropoutVal>0:\n",
    "        model.add(Dropout(DropoutVal))\n",
    "    \n",
    "    # HIDDEN Layer 3: default Param3 = 21\n",
    "    if reg>0:\n",
    "         model.add(tf.keras.layers.Dense(units=Param3, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "    else:\n",
    "         model.add(tf.keras.layers.Dense(units=Param3, kernel_initializer=initializer))\n",
    "    model.add(Activation(activation))\n",
    "    if DropoutVal>0:\n",
    "        model.add(Dropout(DropoutVal))\n",
    "    \n",
    "    if Param4>0:\n",
    "        if reg>0:\n",
    "             model.add(tf.keras.layers.Dense(units=Param4, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "        else:\n",
    "             model.add(tf.keras.layers.Dense(units=Param4, kernel_initializer=initializer))\n",
    "        model.add(Activation(activation))\n",
    "        if DropoutVal>0:\n",
    "            model.add(Dropout(DropoutVal))\n",
    "\n",
    "    # MID HIDDEN Layer 5: default Param5 = 10\n",
    "    if reg>0:\n",
    "         model.add(tf.keras.layers.Dense(units=Param5, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "    else:\n",
    "         model.add(tf.keras.layers.Dense(units=Param5, kernel_initializer=initializer))\n",
    "    model.add(Activation(activation))\n",
    "    if DropoutVal>0:\n",
    "        model.add(Dropout(DropoutVal))\n",
    "\n",
    "    if Param4>0:\n",
    "        if reg>0:\n",
    "             model.add(tf.keras.layers.Dense(units=Param4, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "        else:\n",
    "             model.add(tf.keras.layers.Dense(units=Param4, kernel_initializer=initializer))\n",
    "        model.add(Activation(activation))\n",
    "        if DropoutVal>0:\n",
    "            model.add(Dropout(DropoutVal))\n",
    "            \n",
    "    # HIDDEN Layer 3: default Param3 = 21\n",
    "    if reg>0:\n",
    "         model.add(tf.keras.layers.Dense(units=Param3, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "    else:\n",
    "         model.add(tf.keras.layers.Dense(units=Param3, kernel_initializer=initializer))\n",
    "    model.add(Activation(activation))\n",
    "    if DropoutVal>0:\n",
    "        model.add(Dropout(DropoutVal))\n",
    "\n",
    "    # HIDDEN LAYER 2: default Param2 = 42\n",
    "    if reg>0:\n",
    "         model.add(tf.keras.layers.Dense(units=Param2, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "    else:\n",
    "         model.add(tf.keras.layers.Dense(units=Param2, kernel_initializer=initializer))\n",
    "    model.add(Activation(activation))\n",
    "    if DropoutVal>0:\n",
    "        model.add(Dropout(DropoutVal))\n",
    "    \n",
    "    # OUTPUT layer: default Param1 = 84\n",
    "    if Param1>0:\n",
    "        if reg>0:\n",
    "             model.add(tf.keras.layers.Dense(units=Param1, kernel_regularizer=l2(reg), kernel_initializer=initializer))\n",
    "        else:\n",
    "             model.add(tf.keras.layers.Dense(units=Param1, kernel_initializer=initializer))\n",
    "        model.add(Activation('linear'))\n",
    "        if DropoutVal>0:\n",
    "            model.add(Dropout(DropoutVal))\n",
    "       \n",
    "    # model.compile(optimizer=opt, loss=root_mean_squared_error, metrics=['mse', 'mae', 'mape', 'accuracy'])    \n",
    "    model.compile(optimizer=opt, loss=root_mean_squared_error, metrics=['mse', 'mae', 'mape', 'accuracy'])    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def NeuralNetworkModel(Activation, Param1, Param2, Param3, Param4, Reg, DropoutVal, lr, Optimizer):\n",
    "# HEEL GOED (99553 : 54228) (zonder Contract en Volgnummer) (batchsize=16384)\n",
    "# met 250 EPOCHS lr=0.015, decay=0.05, momentum=0.8: \n",
    "# Correlatie > 0.7 (bestand2) en verrekende contracten NIET weggegooid (bestand 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "# x_train.drop_duplicates(keep='first', inplace=True)\n",
    "# print(x_train.shape)\n",
    "\n",
    "# repetitions = (2,1)\n",
    "# x_train = np.tile(x_train, repetitions)\n",
    "# print(x_train.shape)\n",
    "\n",
    "# define the learning rate change \n",
    "epochs = 10\n",
    "learning_rate = 0.008 # initial learning rate\n",
    "# decay_rate = learning_rate / epochs\n",
    "decay_rate = 0.05\n",
    "momentum = 0.8\n",
    "Reg = 0.0001\n",
    "DropoutVal = 0\n",
    "\n",
    "InnerLayer1 = x_train.shape[1]\n",
    "InnerLayer2 = 50\n",
    "InnerLayer3 = 25\n",
    "InnerLayer4 = 0\n",
    "InnerLayer5 = 0\n",
    "InnerLayer6 = 0\n",
    "InnerLayer7 = 0\n",
    "LatentLayer = 10\n",
    "\n",
    "opt = SelectOptimizer(2, 0.01)\n",
    "kregularizer=regularizers.L2(0.01)\n",
    "bSize = 16384\n",
    "initializer = initializers.GlorotUniform(seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Required Callback Functions\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def exp_decay(epoch):\n",
    "    #lrate = learning_rate * np.exp(-decay_rate*float(epoch))          #exponentieel\n",
    "    lrate = learning_rate * 1/(1 + decay_rate * epoch)                #assymptoot naar 0\n",
    "    #lrate = (((0.002-learning_rate)/epochs) * epoch) + learning_rate   #lineair naar 0\n",
    "    return lrate\n",
    "\n",
    "lr_rate = LearningRateScheduler(exp_decay)\n",
    "\n",
    "class StopTraining(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # if(logs.get('accuracy')>0.99) or (logs.get('val_loss')<0.001):\n",
    "        if (logs.get('val_loss')<0.001):\n",
    "            print(\"\\n99% val_accuracy bereikt, dus algorithme stopt met trainen\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "# een grafiek maken van de learning rate\\\n",
    "x = list(range(0,epochs))\n",
    "y = []\n",
    "for i in x: y.append(exp_decay(i))\n",
    "fig, axs = plt.subplots(figsize=(8, 4))\n",
    "axs.scatter(x, y)\n",
    "axs.set_title('Learning rate als functie van epochs')\n",
    "axs.set(xlabel='Epoch', ylabel='Learning rate')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Voer neuraal netwerk uit   \n",
    "# https://www.deeplearning.ai/ai-notes/initialization/\n",
    "# loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.0107 - val_accuracy: 0.9924\n",
    "# NeuralNetworkModel(\"relu\", 64, 32, 16, 8, 140, 0.0001, 0)\n",
    "#\n",
    "# OVERFITTING VOORKOMEN\n",
    "# Activity Regularization: Penalize the model during training base on the magnitude of the activations.\n",
    "# Weight Constraint: Constrain the magnitude of weights to be within a range or below a limit.\n",
    "# Dropout: Probabilistically remove inputs during training.\n",
    "# Noise: Add statistical noise to inputs during training.\n",
    "# Early Stopping: Monitor model performance on a validation set and stop training when performance degrades.\n",
    "\n",
    "# Veranderende learning rate instellen via een callback functie in Keras\n",
    "# Ook wordt er een plot gemaakt van de learning rate\n",
    "# https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
    "# https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "# lr_callback_1 = tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", patience = 2, mode = \"auto\", min_delta = 0.01, cooldown = 0, min_lr = 0)\n",
    "# tensorboard_callback_1 = tf.keras.callbacks.TensorBoard(log_dir = \"data\", histogram_freq = 1)\n",
    "# modelckpt_callback_1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    monitor = \"val_accuracy\",\n",
    "#    filepath = \"data\",\n",
    "#    verbose = 1,\n",
    "#    mode = \"max\",\n",
    "#    save_weights_only = False,\n",
    "#    save_best_only = True,\n",
    "#    save_freq = \"epoch\"\n",
    "#    )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import History\n",
    "from sklearn.utils import compute_class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "with tf.device(\"cpu:0\"):\n",
    "\n",
    "    # Autoencoder\n",
    "    print(\"\\nDefinieer AutoEncoder Neuraal netwerk\")\n",
    "\n",
    "    reset_random_seeds()\n",
    "    if 1==2:\n",
    "        NNmodel = tf.keras.models.Sequential() \n",
    "        NNmodel.add(tf.keras.Input(shape=(InnerLayer1,)))\n",
    "        if InnerLayer2>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer2, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer3>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer3, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer4>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer4, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer5>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer5, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer6>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer6, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer7>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer7, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        NNmodel.add(tf.keras.layers.Dense(units=LatentLayer, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "        if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer7>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer7, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer6>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer6, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer5>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer5, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer4>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer4, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer3>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer3, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        if InnerLayer2>0:\n",
    "            NNmodel.add(tf.keras.layers.Dense(units=InnerLayer2, activation=\"relu\", kernel_regularizer=l2(Reg)))\n",
    "            if DropoutVal>0: NNmodel.add(Dropout(DropoutVal))\n",
    "        NNmodel.add(tf.keras.layers.Dense(units=InnerLayer1, activation=\"linear\", kernel_regularizer=l2(Reg)))\n",
    "        opt = Nadam(learning_rate=0.01, epsilon=1e-07)\n",
    "        NNmodel.compile(optimizer=opt, loss=root_mean_squared_error, metrics=['mse', 'mae', 'mape', 'accuracy'])    \n",
    "        print(NNmodel.summary())\n",
    "    else:\n",
    "        NNmodel = NeuralNetworkModel(\"relu\", InnerLayer1, InnerLayer2, InnerLayer3, InnerLayer4, LatentLayer, 0, DropoutVal, 0.01, 2)\n",
    "\n",
    "    # learning schedule callback\n",
    "    callbacks_list = [lr_rate, StopTraining()]\n",
    "\n",
    "    # hist = NNmodel.fit(x_train, x_train, batch_size=8192, epochs=20, validation_data=(x_test, x_test), callbacks = [StopTraining(), lr_callback_1, tensorboard_callback_1, modelckpt_callback_1], max_queue_size = 20, use_multiprocessing = True, workers = 24, verbose=1)\n",
    "    hist = NNmodel.fit(x_train, x_train, batch_size=bSize, epochs=epochs, validation_data=(x_test, x_test), callbacks=callbacks_list, verbose=1)\n",
    "    print(NNmodel.summary())\n",
    "    \n",
    "    # plot the autoencoder\n",
    "    plot_model(NNmodel, 'data/autoencoder.png', show_shapes=True)    \n",
    "\n",
    "    x_pred = NNmodel.predict(x_test)\n",
    "\n",
    "    # plot loss and accuracy during training\n",
    "    fig, axs = plt.subplots(2,1,figsize=(10, 10))\n",
    "    axs[0].plot(hist.history['loss'], label='train')\n",
    "    axs[0].plot(hist.history['val_loss'], label='test')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].set(xlabel='Epoch', ylabel='Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(hist.history['mse'], label='train-mse')\n",
    "    axs[1].plot(hist.history['mae'], label='train-mae')\n",
    "    axs[1].plot(hist.history['accuracy'], label='train-accuracy')\n",
    "    axs[1].plot(hist.history['val_mse'], label='test-mse')\n",
    "    axs[1].plot(hist.history['val_mae'], label='test-mae')\n",
    "    axs[1].plot(hist.history['val_accuracy'], label='test-accuracy')\n",
    "    axs[1].set_title('Mse / Mae / Accuracy')\n",
    "    axs[1].set(xlabel='Epoch', ylabel='Mse / Mae / Accuracy')\n",
    "    axs[1].legend()\n",
    "    plt.show() \n",
    "    \n",
    "    # save the weights of the model to file\n",
    "    NNmodel.save('data/encoder.h5')\n",
    "    \n",
    "    print(hist.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(list(hist.history['mse'])[-1])\n",
    "print(list(hist.history['val_mse'])[-1])\n",
    "print(list(hist.history['mae'])[-1])\n",
    "print(list(hist.history['val_mae'])[-1])\n",
    "print(list(hist.history['accuracy'])[-1])\n",
    "print(list(hist.history['val_accuracy'])[-1])\n",
    "print(list(hist.history['mape'])[-1])\n",
    "print(list(hist.history['val_mape'])[-1])\n",
    "print(list(hist.history['loss'])[-1])\n",
    "print(list(hist.history['val_loss'])[-1])\n",
    "print(list(hist.history['lr'])[-1])\n",
    "print(\"\\n\")\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(range(1,10), list(hist.history['loss'])[-9:])\n",
    "print(\"Slope: \" + str(slope))\n",
    "\n",
    "# Calculate Eucledian Distance\n",
    "m = np.sqrt(np.sum(np.square(np.subtract(x_pred, x_test)), axis=1))\n",
    "count = np.count_nonzero(m < 1)\n",
    "print(str(len(x_test)) + \" : \" + str(count))\n",
    "\n",
    "r2 = r2_score(x_test, x_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope: -0.0002583066001534462\n",
    "# 61587 : 60183\n",
    "# 0.8023598236234031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.concat([DID_test, pd.DataFrame(m)], axis=1)\n",
    "DF.head()\n",
    "print(DF[DF[0]>2]['DeclaratieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoBins = 8\n",
    "pdm=pd.DataFrame(m)\n",
    "plt.hist(pdm[pdm[0]<NoBins], bins=NoBins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas het neurale netwerk nogmaals toe, maar nu door de training en test data \n",
    "# steeds te splitsen mbv kFold\n",
    "import os\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# https://stackoverflow.com/questions/39758094/clearing-tensorflow-gpu-memory-after-model-execution\n",
    "from numba import cuda \n",
    "#InteractiveSession.close()\n",
    "\n",
    "TF_CONFIG = ConfigProto()\n",
    "TF_CONFIG.gpu_options.per_process_gpu_memory_fraction = 0.333\n",
    "# TF_CONFIG = ConfigProto(gpu_options=GPUOptions(per_process_gpu_memory_fraction=0.1),allow_soft_placement=True)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#InteractiveSession.close(_self)\n",
    "# df = x_train[[\"ClientId\", \"Periodenummer\", \"GedeclareerdeKostenExclBtw\", \"GemeenteId\", \"nProductID\", \"nAanbiederID\", \"EerdereDeclaraties\"]]\n",
    "\n",
    "\n",
    "if HyperparameterTuning == True:\n",
    "    with tf.device(\"cpu:0\"):\n",
    "        print(\"Dimensie van de X-training: \" + str(np.shape(x_train)) + \" \"+ str(type(x_train)))\n",
    "        print(\"Dimensie van de X-test: \" + str(np.shape(x_test)) + \" \"+ str(type(x_test)))\n",
    "\n",
    "        print(\"\\nOptimalisatie beste Neurale Netwerk: aantal nodes per layer en met kFold)\")\n",
    "        NNscores=[]\n",
    "        ParamScores=[]\n",
    "        ParamScores.append([\"Cnt\", \"LatentLay\", \"Dropout\", \"Param2\", \"lRate\", \"DevSet\", \"Reg\", \"Opt\", \"bSize\", \"Epochs\", \"Count_train\", \"Count_test\", \"R2_train\", \"R2_test\", \"x_train\", \"x_test\", \"vmse\", \"vmsetest\", \"vmae\", \"vmaetest\", \"vacc\", \"vacctest\", \"vloss\", \"vlosstest\", \"vmape\", \"vmapetest\", \"Slope\"])\n",
    "        cnt = 0\n",
    "        Dout = DropoutVal\n",
    "        DevSet = 0\n",
    "        if DevSet>0:\n",
    "            X_train, X_dev = train_test_split(x_train, test_size = DevSet, random_state = 42)\n",
    "        else:\n",
    "            X_train = x_train.copy()\n",
    "        \n",
    "        for LearnRate in range(0, 20, 20):\n",
    "            #print(\"Optimalisatie Param 1; Learning Rate: \" + str(float(LearnRate/100000)))\n",
    "            lRate = LearnRate/100000\n",
    "            # for dSet in range(1, 2, 1):\n",
    "                # DevSet = dSet/40\n",
    "                # X_train, X_dev = train_test_split(x_train, test_size = DevSet, random_state = 42)\n",
    "            for DropoutV in range(10, 100, 10):\n",
    "                Dout = float(DropoutV/100)\n",
    "                print(\"Dropout: \" + str(Dout) )\n",
    "                for iReg in range(0,1,1):\n",
    "                    Reg = iReg/100000\n",
    "                    for Opt in range(2,3,1):\n",
    "                        for Param2 in range(170,171,1):\n",
    "                            bSize = 16*1024\n",
    "                            for LatentLay in range(16,17,1):\n",
    "                                #InteractiveSession.close(_self)\n",
    "                                sess = InteractiveSession(config=TF_CONFIG)\n",
    "                                reset_random_seeds()\n",
    "\n",
    "                                # def NeuralNetworkModel(Activation, Param1, Param2, Param3, Param4, Param5, Reg, DropoutVal, lr, Optimizer):\n",
    "                                NNmodel = NeuralNetworkModel(\"relu\", InnerLayer1, InnerLayer2, InnerLayer3, InnerLayer4, LatentLay, Reg, Dout, lRate, Opt)\n",
    "                                cnt=cnt+1\n",
    "                                NNscores.clear()\n",
    "                                print(\"\\nParameters bij onderstaande resultaten\\nLearningRate:\" + str(lRate) + \"  Dropout:\" + str(Dout) + \"  Param2:\" + str(Param2) + \"  LatentLayer:\" + str(LatentLay) + \"  DevSet:\" + str(DevSet) + \"  Reg:\" + str(Reg) + \"  Opt:\" + str(Opt) + \"  Batchsize:\" + str(bSize) + \"  Epochs:\" + str(epochs))\n",
    "\n",
    "                                hist = NNmodel.fit(X_train, X_train, batch_size=bSize, epochs=epochs, validation_data=(x_test, x_test), callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "                                vmse = list(hist.history['mse'])[-1]\n",
    "                                vmsetest = list(hist.history['val_mse'])[-1]\n",
    "                                vmae = list(hist.history['mae'])[-1]\n",
    "                                vmaetest = list(hist.history['val_mae'])[-1]\n",
    "                                vacc = list(hist.history['accuracy'])[-1]\n",
    "                                vacctest = list(hist.history['val_accuracy'])[-1]\n",
    "                                vloss = list(hist.history['loss'])[-1]\n",
    "                                vlosstest = list(hist.history['val_loss'])[-1]\n",
    "                                vmape = list(hist.history['mape'])[-1]\n",
    "                                vmapetest = list(hist.history['val_mape'])[-1]\n",
    "                                vlr = list(hist.history['lr'])[-1]\n",
    "\n",
    "                                xtrain_pred = NNmodel.predict(X_train)\n",
    "                                xtest_pred = NNmodel.predict(x_test)\n",
    "\n",
    "                                # Bepaal de R2 score op de training set en bepaals hoeveel records een EUCL DIST < 1 hebben\n",
    "                                m_train = np.sqrt(np.sum(np.square(np.subtract(xtrain_pred, X_train)), axis=1))\n",
    "                                count_train = np.count_nonzero(m_train < 1)/X_train.shape[0]\n",
    "                                r2_train = r2_score(X_train, xtrain_pred)\n",
    "                                \n",
    "                                # Bepaal de R2 score op de test set en bepaals hoeveel records een EUCL DIST < 1 hebben\n",
    "                                m_test = np.sqrt(np.sum(np.square(np.subtract(xtest_pred, x_test)), axis=1))\n",
    "                                count_test = np.count_nonzero(m_test < 1)/x_test.shape[0]\n",
    "                                r2_test = r2_score(x_test, xtest_pred)\n",
    "                                \n",
    "                                slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(range(1,10), list(hist.history['loss'])[-9:])\n",
    "\n",
    "                                print (\"Loss: \" + str(vloss) + \"  Loss test-set: \" + str(vlosstest))\n",
    "                                ParamScores.append([cnt, LatentLay, Dout, Param2, vlr, DevSet, Reg, Opt, bSize, epochs, count_train, count_test, r2_train, r2_test, X_train.shape, x_test.shape, vmse, vmsetest, vmae, vmaetest, vacc, vacctest, vloss, vlosstest, vmape, vmapetest, slope])\n",
    "\n",
    "                                #device = cuda.get_current_device()\n",
    "                                #device.reset()\n",
    "                                sess.close()\n",
    "                                #cuda.select_device(0)\n",
    "                                #cuda.close()\n",
    "\n",
    "    fig, axs = plt.subplots(2,1,figsize=(10, 5))\n",
    "    axs[0].plot(hist.history['loss'], label='train')\n",
    "    axs[0].plot(hist.history['val_loss'], label='test')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].set(xlabel='epoch', ylabel='loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(hist.history['mse'], label='train-mse')\n",
    "    axs[1].plot(hist.history['mae'], label='train-mae')\n",
    "    axs[1].plot(hist.history['accuracy'], label='train-accuracy')\n",
    "    axs[1].plot(hist.history['val_mse'], label='test-mse')\n",
    "    axs[1].plot(hist.history['val_mae'], label='test-mae')\n",
    "    axs[1].plot(hist.history['val_accuracy'], label='test-accuracy')\n",
    "    axs[1].set_title('Mse / Mae / Accuracy')\n",
    "    axs[1].set(xlabel='Epoch', ylabel='Mse / Mae / Accuracy')\n",
    "    axs[1].legend()\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HyperparameterTuning == True:\n",
    "    df = pd.DataFrame(ParamScores)\n",
    "    writer = pd.ExcelWriter('test_l2.xlsx', engine='xlsxwriter')\n",
    "    df.to_excel(writer, sheet_name='testresults', index=False)\n",
    "    writer.save()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
